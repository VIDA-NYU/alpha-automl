{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the class `AutoMLClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_automl  import AutoMLClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Pipelines for CSV Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are generating pipelines for a CSV dataset. The sentiment dataset is used for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'tmp/'\n",
    "train_dataset = pd.read_csv('datasets/sentiment/train_data.csv')\n",
    "test_dataset = pd.read_csv('datasets/sentiment/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the target column from the features for the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Ghana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Greece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Grenada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Guatemala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Guinea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text Time of Tweet  \\\n",
       "0                    I`d have responded, if I were going       morning   \n",
       "1          Sooo SAD I will miss you here in San Diego!!!          noon   \n",
       "2                              my boss is bullying me...         night   \n",
       "3                         what interview! leave me alone       morning   \n",
       "4       Sons of ****, why couldn`t they put them on t...          noon   \n",
       "...                                                  ...           ...   \n",
       "27476   wish we could come see u on Denver  husband l...         night   \n",
       "27477   I`ve wondered about rake to.  The client has ...       morning   \n",
       "27478   Yay good for both of you. Enjoy the break - y...          noon   \n",
       "27479                         But it was worth it  ****.         night   \n",
       "27480     All this flirting going on - The ATG smiles...       morning   \n",
       "\n",
       "      Age of User      Country  \n",
       "0            0-20  Afghanistan  \n",
       "1           21-30      Albania  \n",
       "2           31-45      Algeria  \n",
       "3           46-60      Andorra  \n",
       "4           60-70       Angola  \n",
       "...           ...          ...  \n",
       "27476       31-45        Ghana  \n",
       "27477       46-60       Greece  \n",
       "27478       60-70      Grenada  \n",
       "27479      70-100    Guatemala  \n",
       "27480        0-20       Guinea  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = 'sentiment'\n",
    "X_train = train_dataset.drop(columns=[target_column])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the target column for the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment\n",
       "0       neutral\n",
       "1      negative\n",
       "2      negative\n",
       "3      negative\n",
       "4      negative\n",
       "...         ...\n",
       "27476  negative\n",
       "27477  negative\n",
       "27478  positive\n",
       "27479  positive\n",
       "27480   neutral\n",
       "\n",
       "[27481 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_dataset[[target_column]]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding New Primitives into AlphaAutoML's Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoMLClassifier(output_path, time_bound=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: xlm-roberta-base\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/xlm-roberta-base HTTP/1.1\" 200 2929\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/.gitattributes HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/model.safetensors HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/sentencepiece.bpe.model HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/tokenizer.json HTTP/1.1\" 200 0\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/rlopez/.cache/torch/sentence_transformers/xlm-roberta-base. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/rlopez/.cache/torch/sentence_transformers/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from my_module import MyEmbedder\n",
    "\n",
    "my_embedder = MyEmbedder()\n",
    "automl.add_primitives([(my_embedder, 'TEXT_ENCODER')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching  Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datamart_profiler.core:Setting column names from header\n",
      "INFO:datamart_profiler.core:Identifying types, 4 columns...\n",
      "INFO:datamart_profiler.core:Processing column 0 'text'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datamart_profiler.core:Column type http://schema.org/Text [http://schema.org/Text]\n",
      "INFO:datamart_profiler.core:Processing column 1 'Time of Tweet'...\n",
      "INFO:datamart_profiler.core:Column type http://schema.org/Text [http://schema.org/Enumeration]\n",
      "INFO:datamart_profiler.core:Processing column 2 'Age of User'...\n",
      "INFO:datamart_profiler.core:Column type http://schema.org/Text [http://schema.org/Enumeration]\n",
      "INFO:datamart_profiler.core:Processing column 3 'Country'...\n",
      "INFO:datamart_profiler.core:Column type http://schema.org/Text [http://schema.org/Enumeration]\n",
      "INFO:alpha_automl.data_profiler:Results of profiling data: non-numeric features = dict_keys(['TEXT_ENCODER', 'CATEGORICAL_ENCODER']), useless columns = [], missing values = True\n",
      "INFO:alpha_automl.utils:Sampling down data from 27481 to 2000\n",
      "INFO:alpha_automl.pipeline_synthesis.setup_search:Creating a manual grammar\n",
      "INFO:alpha_automl.primitive_loader:Hierarchy of all primitives loaded\n",
      "INFO:alpha_automl.grammar_loader:Creating task grammar for task CLASSIFICATION_TASK\n",
      "INFO:alpha_automl.grammar_loader:Task grammar: Grammar with 27 productions (start state = S)\n",
      "    S -> IMPUTATION ENCODERS FEATURE_SCALING CLASSIFICATION\n",
      "    ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "    IMPUTATION -> 'sklearn.impute.SimpleImputer'\n",
      "    FEATURE_SCALING -> 'sklearn.preprocessing.MaxAbsScaler'\n",
      "    FEATURE_SCALING -> 'sklearn.preprocessing.RobustScaler'\n",
      "    FEATURE_SCALING -> 'sklearn.preprocessing.StandardScaler'\n",
      "    FEATURE_SCALING -> 'E'\n",
      "    TEXT_ENCODER -> 'sklearn.feature_extraction.text.CountVectorizer'\n",
      "    TEXT_ENCODER -> 'sklearn.feature_extraction.text.TfidfVectorizer'\n",
      "    TEXT_ENCODER -> 'my_module.MyEmbedder'\n",
      "    CATEGORICAL_ENCODER -> 'sklearn.preprocessing.OneHotEncoder'\n",
      "    CLASSIFICATION -> 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'\n",
      "    CLASSIFICATION -> 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'\n",
      "    CLASSIFICATION -> 'sklearn.ensemble.BaggingClassifier'\n",
      "    CLASSIFICATION -> 'sklearn.ensemble.ExtraTreesClassifier'\n",
      "    CLASSIFICATION -> 'sklearn.ensemble.GradientBoostingClassifier'\n",
      "    CLASSIFICATION -> 'sklearn.ensemble.RandomForestClassifier'\n",
      "    CLASSIFICATION -> 'sklearn.naive_bayes.BernoulliNB'\n",
      "    CLASSIFICATION -> 'sklearn.naive_bayes.GaussianNB'\n",
      "    CLASSIFICATION -> 'sklearn.naive_bayes.MultinomialNB'\n",
      "    CLASSIFICATION -> 'sklearn.neighbors.KNeighborsClassifier'\n",
      "    CLASSIFICATION -> 'sklearn.linear_model.LogisticRegression'\n",
      "    CLASSIFICATION -> 'sklearn.linear_model.PassiveAggressiveClassifier'\n",
      "    CLASSIFICATION -> 'sklearn.linear_model.SGDClassifier'\n",
      "    CLASSIFICATION -> 'sklearn.svm.LinearSVC'\n",
      "    CLASSIFICATION -> 'sklearn.svm.SVC'\n",
      "    CLASSIFICATION -> 'sklearn.tree.DecisionTreeClassifier'\n",
      "INFO:alpha_automl.grammar_loader:Creating game grammar\n",
      "INFO:alpha_automl.pipeline_search.Coach:------ITER 1------\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 1\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: S\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.50359595\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 2\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: S\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: S -> IMPUTATION ENCODERS FEATURE_SCALING CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.5035709\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 3\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: S\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: S -> IMPUTATION ENCODERS FEATURE_SCALING CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.50347376\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 4\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: S\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: S -> IMPUTATION ENCODERS FEATURE_SCALING CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.503646\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 5\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: S\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: S -> IMPUTATION ENCODERS FEATURE_SCALING CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.50355506\n",
      "INFO:alpha_automl.pipeline_search.Coach:COACH ACTION 0\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: S -> IMPUTATION ENCODERS FEATURE_SCALING CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 1\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.5033821\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 2\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.50364906\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 3\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_synthesis.pipeline_builder:New pipelined created:\n",
      "Pipeline(steps=[('sklearn.impute.SimpleImputer',\n",
      "                 SimpleImputer(strategy='most_frequent')),\n",
      "                ('sklearn.compose.ColumnTransformer',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('sklearn.feature_extraction.text.CountVectorizer-text',\n",
      "                                                  CountVectorizer(), 0),\n",
      "                                                 ('sklearn.preprocessing.OneHotEncoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  [1, 2, 3])])),\n",
      "                ('sklearn.preprocessing.MaxAbsScaler', MaxAbsScaler()),\n",
      "                ('sklearn.discriminant_analysis.LinearDiscriminantAnalysis',\n",
      "                 LinearDiscriminantAnalysis())])\n",
      "WARNING:alpha_automl.scorer:Exception scoring a pipeline\n",
      "WARNING:alpha_automl.scorer:Detailed error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rlopez/D3M/alpha-automl/alpha_automl/scorer.py\", line 73, in score_pipeline\n",
      "    scores = cross_val_score(pipeline, X, y, cv=splitting_strategy, scoring=scoring, error_score='raise')\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 515, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 575, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/base.py\", line 565, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 845, in check_array\n",
      "    array = _ensure_sparse_format(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 522, in _ensure_sparse_format\n",
      "    raise TypeError(\n",
      "TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 4\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_synthesis.pipeline_builder:New pipelined created:\n",
      "Pipeline(steps=[('sklearn.impute.SimpleImputer',\n",
      "                 SimpleImputer(strategy='most_frequent')),\n",
      "                ('sklearn.compose.ColumnTransformer',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('sklearn.feature_extraction.text.CountVectorizer-text',\n",
      "                                                  CountVectorizer(), 0),\n",
      "                                                 ('sklearn.preprocessing.OneHotEncoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  [1, 2, 3])])),\n",
      "                ('sklearn.preprocessing.MaxAbsScaler', MaxAbsScaler()),\n",
      "                ('sklearn.naive_bayes.MultinomialNB', MultinomialNB())])\n",
      "INFO:alpha_automl.scorer:Score: 0.496\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 5\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|ENCODERS|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.linear_model.LogisticRegression\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.linear_model.LogisticRegression\n",
      "INFO:alpha_automl.automl_manager:Found new pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:alpha_automl.automl_api:Found pipeline, time=0:00:09, scoring...\n",
      "INFO:alpha_automl.pipeline_synthesis.pipeline_builder:New pipelined created:\n",
      "Pipeline(steps=[('sklearn.impute.SimpleImputer',\n",
      "                 SimpleImputer(strategy='most_frequent')),\n",
      "                ('sklearn.compose.ColumnTransformer',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('sklearn.feature_extraction.text.CountVectorizer-text',\n",
      "                                                  CountVectorizer(), 0),\n",
      "                                                 ('sklearn.preprocessing.OneHotEncoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  [1, 2, 3])])),\n",
      "                ('sklearn.preprocessing.MaxAbsScaler', MaxAbsScaler()),\n",
      "                ('sklearn.linear_model.LogisticRegression',\n",
      "                 LogisticRegression())])\n",
      "INFO:alpha_automl.scorer:Score: 0.552\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:findwin 1\n",
      "INFO:alpha_automl.pipeline_search.Coach:COACH ACTION 1\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: ENCODERS -> TEXT_ENCODER CATEGORICAL_ENCODER\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 1\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.linear_model.PassiveAggressiveClassifier\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.linear_model.PassiveAggressiveClassifier\n",
      "INFO:alpha_automl.pipeline_synthesis.pipeline_builder:New pipelined created:\n",
      "Pipeline(steps=[('sklearn.impute.SimpleImputer',\n",
      "                 SimpleImputer(strategy='most_frequent')),\n",
      "                ('sklearn.compose.ColumnTransformer',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('sklearn.feature_extraction.text.CountVectorizer-text',\n",
      "                                                  CountVectorizer(), 0),\n",
      "                                                 ('sklearn.preprocessing.OneHotEncoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  [1, 2, 3])])),\n",
      "                ('sklearn.preprocessing.MaxAbsScaler', MaxAbsScaler()),\n",
      "                ('sklearn.linear_model.PassiveAggressiveClassifier',\n",
      "                 PassiveAggressiveClassifier())])\n",
      "INFO:alpha_automl.scorer:Score: 0.54\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 2\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.50346845\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 3\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 4\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 5\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: IMPUTATION|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.Coach:COACH ACTION 2\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: IMPUTATION -> sklearn.impute.SimpleImputer\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 1\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.5036396\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 2\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 3\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.naive_bayes.MultinomialNB\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 4\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.5036276\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 5\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|FEATURE_SCALING|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.CountVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.CountVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.Coach:COACH ACTION 3\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: FEATURE_SCALING -> sklearn.preprocessing.MaxAbsScaler\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 1\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.TfidfVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.TfidfVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.5035404\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 2\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> sklearn.feature_extraction.text.TfidfVectorizer\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.TfidfVectorizer|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|sklearn.feature_extraction.text.TfidfVectorizer|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_synthesis.pipeline_builder:New pipelined created:\n",
      "Pipeline(steps=[('sklearn.impute.SimpleImputer',\n",
      "                 SimpleImputer(strategy='most_frequent')),\n",
      "                ('sklearn.compose.ColumnTransformer',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('sklearn.feature_extraction.text.TfidfVectorizer-text',\n",
      "                                                  TfidfVectorizer(), 0),\n",
      "                                                 ('sklearn.preprocessing.OneHotEncoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  [1, 2, 3])])),\n",
      "                ('sklearn.preprocessing.MaxAbsScaler', MaxAbsScaler()),\n",
      "                ('sklearn.discriminant_analysis.LinearDiscriminantAnalysis',\n",
      "                 LinearDiscriminantAnalysis())])\n",
      "WARNING:alpha_automl.scorer:Exception scoring a pipeline\n",
      "WARNING:alpha_automl.scorer:Detailed error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rlopez/D3M/alpha-automl/alpha_automl/scorer.py\", line 73, in score_pipeline\n",
      "    scores = cross_val_score(pipeline, X, y, cv=splitting_strategy, scoring=scoring, error_score='raise')\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 515, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/discriminant_analysis.py\", line 575, in fit\n",
      "    X, y = self._validate_data(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/base.py\", line 565, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 845, in check_array\n",
      "    array = _ensure_sparse_format(\n",
      "  File \"/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 522, in _ensure_sparse_format\n",
      "    raise TypeError(\n",
      "TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 3\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> my_module.MyEmbedder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|my_module.MyEmbedder|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.MCTS:Prediction 0.5036264\n",
      "INFO:alpha_automl.pipeline_search.MCTS:MCTS SIMULATION 4\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|CLASSIFICATION\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CLASSIFICATION -> sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|TEXT_ENCODER|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: TEXT_ENCODER -> my_module.MyEmbedder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|my_module.MyEmbedder|CATEGORICAL_ENCODER|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineLogic:MOVE ACTION: CATEGORICAL_ENCODER -> sklearn.preprocessing.OneHotEncoder\n",
      "INFO:alpha_automl.pipeline_search.pipeline.PipelineGame:PIPELINE: sklearn.impute.SimpleImputer|my_module.MyEmbedder|sklearn.preprocessing.OneHotEncoder|sklearn.preprocessing.MaxAbsScaler|sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
      "INFO:alpha_automl.pipeline_synthesis.pipeline_builder:New pipelined created:\n",
      "Pipeline(steps=[('sklearn.impute.SimpleImputer',\n",
      "                 SimpleImputer(strategy='most_frequent')),\n",
      "                ('sklearn.compose.ColumnTransformer',\n",
      "                 ColumnTransformer(remainder='passthrough',\n",
      "                                   transformers=[('my_module.MyEmbedder-text',\n",
      "                                                  MyEmbedder(), 0),\n",
      "                                                 ('sklearn.preprocessing.OneHotEncoder',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  [1, 2, 3])])),\n",
      "                ('sklearn.preprocessing.MaxAbsScaler', MaxAbsScaler()),\n",
      "                ('sklearn.discriminant_analysis.LinearDiscriminantAnalysis',\n",
      "                 LinearDiscriminantAnalysis())])\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: xlm-roberta-base\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "INFO:alpha_automl.scorer:Score: 0.5919080192111774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:alpha_automl.automl_manager:Pipeline scored successfully, score=0.5919080192111774\n",
      "INFO:alpha_automl.automl_api:Scored pipeline, score=0.5919080192111774\n",
      "INFO:alpha_automl.automl_manager:Found new pipeline\n",
      "INFO:alpha_automl.automl_api:Found pipeline, time=0:00:10, scoring...\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/xlm-roberta-base HTTP/1.1\" 200 2929\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/.gitattributes HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/model.safetensors HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/sentencepiece.bpe.model HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/tokenizer.json HTTP/1.1\" 200 0\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/rlopez/.cache/torch/sentence_transformers/xlm-roberta-base. Creating a new one with MEAN pooling.\n",
      "INFO:alpha_automl.scorer:Score: 0.6787949352350459\n",
      "INFO:alpha_automl.automl_manager:Pipeline scored successfully, score=0.6787949352350459\n",
      "INFO:alpha_automl.automl_api:Scored pipeline, score=0.6787949352350459\n",
      "INFO:alpha_automl.automl_manager:Found new pipeline\n",
      "INFO:alpha_automl.automl_api:Found pipeline, time=0:00:12, scoring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:alpha_automl.scorer:Score: 0.621161402998108\n",
      "INFO:alpha_automl.automl_manager:Pipeline scored successfully, score=0.621161402998108\n",
      "INFO:alpha_automl.automl_api:Scored pipeline, score=0.621161402998108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlopez/D3M/alpha-automl/alpha_automl/pipeline_search/pipeline/NNet.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  board = Variable(board, volatile=True)\n",
      "/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/rlopez/D3M/alpha-automl/alpha_automl/pipeline_search/pipeline/NNet.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  board = Variable(board, volatile=True)\n",
      "/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/rlopez/D3M/alpha-automl/alpha_automl/pipeline_search/pipeline/NNet.py:103: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  board = Variable(board, volatile=True)\n",
      "/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Some weights of the model checkpoint at /Users/rlopez/.cache/torch/sentence_transformers/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: xlm-roberta-base\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/xlm-roberta-base HTTP/1.1\" 200 2929\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/.gitattributes HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/model.safetensors HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/sentencepiece.bpe.model HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /xlm-roberta-base/resolve/42f548f32366559214515ec137cdd16002968bf6/tokenizer.json HTTP/1.1\" 200 0\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/rlopez/.cache/torch/sentence_transformers/xlm-roberta-base. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/rlopez/.cache/torch/sentence_transformers/xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:  38%|███▊      | 18/47 [00:44<01:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:alpha_automl.pipeline_synthesis.setup_search:Receiving signal, terminating process\n",
      "INFO:alpha_automl.automl_manager:Found 3 pipelines\n",
      "INFO:alpha_automl.automl_manager:Search done\n",
      "INFO:alpha_automl.automl_api:Found 3 pipelines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "automl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pipeline search is complete, we can display the leaderboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7c47f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_7c47f_level0_col0\" class=\"col_heading level0 col0\" >ranking</th>\n",
       "      <th id=\"T_7c47f_level0_col1\" class=\"col_heading level0 col1\" >pipeline</th>\n",
       "      <th id=\"T_7c47f_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7c47f_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_7c47f_row0_col1\" class=\"data row0 col1\" >SimpleImputer, ColumnTransformer, CountVectorizer, OneHotEncoder, MaxAbsScaler, LogisticRegression</td>\n",
       "      <td id=\"T_7c47f_row0_col2\" class=\"data row0 col2\" >0.678795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7c47f_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_7c47f_row1_col1\" class=\"data row1 col1\" >SimpleImputer, ColumnTransformer, CountVectorizer, OneHotEncoder, MaxAbsScaler, PassiveAggressiveClassifier</td>\n",
       "      <td id=\"T_7c47f_row1_col2\" class=\"data row1 col2\" >0.621161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7c47f_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_7c47f_row2_col1\" class=\"data row2 col1\" >SimpleImputer, ColumnTransformer, CountVectorizer, OneHotEncoder, MaxAbsScaler, MultinomialNB</td>\n",
       "      <td id=\"T_7c47f_row2_col2\" class=\"data row2 col2\" >0.591908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d74e6190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.plot_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the target column from the features for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Nicaragua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>All alone in this old house again.  Thanks for...</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Niger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>I know what you mean. My little dog is sinkin...</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>_sutra what is your next youtube video gonna b...</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>North Macedonia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3534 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text Time of Tweet  \\\n",
       "0     Last session of the day  http://twitpic.com/67ezh       morning   \n",
       "1      Shanghai is also really exciting (precisely -...          noon   \n",
       "2     Recession hit Veronique Branquinho, she has to...         night   \n",
       "3                                           happy bday!       morning   \n",
       "4                http://twitpic.com/4w75p - I like it!!          noon   \n",
       "...                                                 ...           ...   \n",
       "3529  its at 3 am, im very tired but i can`t sleep  ...          noon   \n",
       "3530  All alone in this old house again.  Thanks for...         night   \n",
       "3531   I know what you mean. My little dog is sinkin...       morning   \n",
       "3532  _sutra what is your next youtube video gonna b...          noon   \n",
       "3533   http://twitpic.com/4woj2 - omgssh  ang cute n...         night   \n",
       "\n",
       "     Age of User          Country  \n",
       "0           0-20      Afghanistan  \n",
       "1          21-30          Albania  \n",
       "2          31-45          Algeria  \n",
       "3          46-60          Andorra  \n",
       "4          60-70           Angola  \n",
       "...          ...              ...  \n",
       "3529       21-30        Nicaragua  \n",
       "3530       31-45            Niger  \n",
       "3531       46-60          Nigeria  \n",
       "3532       60-70      North Korea  \n",
       "3533      70-100  North Macedonia  \n",
       "\n",
       "[3534 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_dataset.drop(columns=[target_column])\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the target column for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3534 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment\n",
       "0      neutral\n",
       "1     positive\n",
       "2     negative\n",
       "3     positive\n",
       "4     positive\n",
       "...        ...\n",
       "3529  negative\n",
       "3530  positive\n",
       "3531  negative\n",
       "3532  positive\n",
       "3533  positive\n",
       "\n",
       "[3534 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_dataset[[target_column]]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline predictions are accessed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'positive', 'negative', ..., 'negative', 'positive',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = automl.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline can be evaluated against a held out dataset with the function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:alpha_automl.automl_api:Metric: accuracy, Score: 0.6825127334465195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlopez/opt/anaconda3/envs/alphaautoml/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 'accuracy', 'score': 0.6825127334465195}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing pipelines using Pipeline Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore the produced pipelines, we can use [PipelineProfiler](https://github.com/VIDA-NYU/PipelineVis). PipelineProfiler is a visualization that enables users to compare and explore the pipelines generated by the AlphaAutoML system.\n",
    "\n",
    "After the pipeline search process is completed, we can use PipelineProfiler with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.plot_comparison_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about how to use PipelineProfiler, click [here](https://towardsdatascience.com/exploring-auto-sklearn-models-with-pipelineprofiler-5b2c54136044). There is also a video demo available [here](https://www.youtube.com/watch?v=2WSYoaxLLJ8)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
