{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of OpenML Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "systems = ['AutoGluon', 'AutoWEKA', 'Auto-Sklearn', 'H2O', 'TPOT', 'AlphaD3M', 'Alpha-AutoML']\n",
    "performances = pd.read_csv('results/results.csv')\n",
    "performances = performances[['id', 'framework', 'type', 'result']]\n",
    "performances = pd.pivot_table(performances, index=['id', 'type'], columns='framework', values='result')\n",
    "performances = performances.reset_index()\n",
    "performances.columns.name = None\n",
    "performances.rename(columns={'id': 'Dataset', 'type': 'Type', 'H2OAutoML': 'H2O', 'autosklearn': 'Auto-Sklearn'}, inplace=True)\n",
    "performances = performances[['Dataset', 'Type'] + systems]\n",
    "performances = performances.replace('openml.org/t/','task_', regex=True)\n",
    "performances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def calculate_rank(performances):\n",
    "    performances_t = performances.T\n",
    "    performances_t.columns = performances_t.loc['Dataset'].tolist() # Set the datasets as column names\n",
    "    all_ranks = []\n",
    "    \n",
    "    for dataset in performances_t.columns:\n",
    "        ranks_by_dataset = pd.DataFrame({dataset: performances_t[dataset]})\n",
    "        ranks_by_dataset.drop(['Dataset', 'Type'], inplace=True) # Remove 'Dataset', and 'Type'\n",
    "        ranks_by_dataset = ranks_by_dataset.rank(ascending=False, method='min')\n",
    "        worst_rank = float(ranks_by_dataset.shape[0]) # Number of AutoML Systems\n",
    "        ranks_by_dataset.fillna(worst_rank, inplace=True) # Add the worst rank to the systems that didn't produce pipelines\n",
    "        all_ranks.append(ranks_by_dataset)\n",
    "        \n",
    "    all_ranks = pd.concat(all_ranks, axis=1, join='inner')\n",
    "    \n",
    "    all_ranks['average_rank'] = all_ranks.mean(axis=1) # Add a column with average rank\n",
    "    all_ranks = all_ranks.round({'average_rank': 2})\n",
    "    \n",
    "    return all_ranks\n",
    "\n",
    "def generate_latex(all_performances, file_name):\n",
    "    performances = all_performances.copy(deep=True)\n",
    "    try:\n",
    "        performances.drop(columns=['Type'], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    performances.to_latex(f'{file_name}.tex', float_format='%.2f', index=False, na_rep='-')\n",
    "    print(f'Latex generated at {file_name}.tex file.')\n",
    "\n",
    "def calculate_gain(all_performances, target_automl='Alpha-AutoML', worst_score=0):\n",
    "    performances = all_performances.copy(deep=True)\n",
    "    other_systems = [s for s in systems if s != target_automl]\n",
    "    performances['Target_AutoML'] = performances[target_automl].fillna(worst_score)\n",
    "    performances['Others_AutoML'] = performances[other_systems].fillna(worst_score).mean(axis=1)\n",
    "    performances['Gain'] = performances['Target_AutoML'] - performances['Others_AutoML']\n",
    "    performances.drop(columns=['Target_AutoML', 'Others_AutoML'], inplace=True)\n",
    "    \n",
    "    return performances.round(2)\n",
    "\n",
    "def calculate_difference(all_performances, target_automl='Alpha-AutoML', worst_score=0):\n",
    "    performances = all_performances.copy(deep=True)\n",
    "    other_systems = [s for s in systems if s != target_automl]\n",
    "    performances['Target_AutoML'] = performances[target_automl].fillna(worst_score)\n",
    "    performances['Best_Performance'] = performances[other_systems].max(axis=1)\n",
    "    performances['Difference'] = performances['Target_AutoML'] - performances['Best_Performance']\n",
    "    performances.drop(columns=['Target_AutoML', 'Best_Performance'], inplace=True)\n",
    "    \n",
    "    return performances.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Average Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = calculate_rank(performances)\n",
    "ranks.sort_values(by='average_rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Difference with the Best Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = calculate_difference(performances)\n",
    "difference.sort_values(by='Difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gains = calculate_gain(performances)\n",
    "gains.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(gains['Gain'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latex(gains, 'gains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances[systems] = performances[systems].apply(lambda x: x/x.max(), axis=1)\n",
    "performances = performances.round(2)\n",
    "performances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latex(performances, 'normalized_performances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dataframe to the format of altair lib\n",
    "performances = pd.melt(performances, id_vars=['Dataset', 'Type'], var_name='AutoML', value_name='Performance')\n",
    "performances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performances(source):\n",
    "    domain = systems\n",
    "    color_range = ['#f7b97c', '#f58517', '#e7ba52', '#e45857', '#d67196', '#ccf77c', '#396cb0']\n",
    "    \n",
    "    return alt.Chart(source, title=\"\").mark_point(filled=True, size=32).encode(\n",
    "        alt.X(\n",
    "            'Performance:Q',\n",
    "            title=\"Accuracy\",\n",
    "            scale=alt.Scale(zero=False),\n",
    "            axis=alt.Axis(grid=False)\n",
    "        ),\n",
    "        alt.Y(\n",
    "            'Dataset:N',\n",
    "            title=\"\",\n",
    "            sort='-x',\n",
    "            axis=alt.Axis(grid=True)\n",
    "        ),\n",
    "        #color=alt.Color('AutoML:N', legend=alt.Legend(title=\"AutoML\")),\n",
    "        color=alt.Color('AutoML:N', scale=alt.Scale(domain=domain, range=color_range), legend=alt.Legend(title=\"AutoML\")),\n",
    "        row=alt.Row(\n",
    "            'Type:N',\n",
    "            title=\"\",\n",
    "            sort=alt.EncodingSortField(field='yield', op='sum', order='descending'),\n",
    "        )\n",
    "    ).properties(\n",
    "        height=alt.Step(12),\n",
    "        width=250\n",
    "    )\n",
    "# .configure_view(stroke=\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = performances[(performances['Type']=='binary')]\n",
    "chart1 = plot_performances(df_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiclass = performances[(performances['Type']=='multiclass')]\n",
    "chart2 = plot_performances(df_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.hconcat(chart1, chart2).configure_view(stroke='transparent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
